{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nonnegative Matrix Factorization (NMF)\n",
    "# for Time Series Forecasting and Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import sys  \n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0) Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.io.parsers.read_csv('./data/LD2011_2014.txt', sep=\";\", index_col=0, header=0, low_memory=False, decimal=',')\n",
    "df = data\n",
    "df = df.iloc[2*96:, :]\n",
    "df = df.iloc[:-1, :]\n",
    "df = df.iloc[:-3*96, :]\n",
    "\n",
    "df.index = pd.to_datetime(df.index, format='%Y-%m-%d')\n",
    "df = df.groupby(pd.Grouper(freq='W-MON')).sum()\n",
    "df = df.iloc[:-1, :]\n",
    "\n",
    "print(df.transpose())\n",
    "\n",
    "periods_to_forecast = 4\n",
    "\n",
    "X_original = df.transpose().values\n",
    "n = np.shape(X_original)[0]\n",
    "d = np.shape(X_original)[1]\n",
    "\n",
    "d0 = d-periods_to_forecast\n",
    "X_real = X_original.copy()\n",
    "\n",
    "X_train = X_original[:, :d0]\n",
    "X_test = X_original[:, d0:d0+periods_to_forecast]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Latent Clustered Forecast (LCF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, './latent clustered forecast')\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import make_scorer, recall_score, precision_score, mean_absolute_error\n",
    "import multiprocessing as mp\n",
    "import include_lcf as nmf\n",
    "\n",
    "sys.setrecursionlimit(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_directory = os.getcwd()\n",
    "matrix_dir = os.path.join(current_directory, r'matrix')\n",
    "archetypes_dir = os.path.join(current_directory, r'archetypes')\n",
    "trajectories_dir = os.path.join(current_directory, r'trajectories')\n",
    "if not os.path.exists(matrix_dir):\n",
    "    os.makedirs(matrix_dir)\n",
    "if not os.path.exists(archetypes_dir):\n",
    "    os.makedirs(archetypes_dir)\n",
    "if not os.path.exists(trajectories_dir):\n",
    "    os.makedirs(trajectories_dir)\n",
    "\n",
    "# %%\n",
    "\n",
    "max_depth_list = [4]\n",
    "time_windows_list = [4]\n",
    "list_p = [4]\n",
    "\n",
    "best_error = 1000000\n",
    "\n",
    "f = open(\"output_file_calibration.out\", 'w')\n",
    "\n",
    "for p in list_p:\n",
    "    \n",
    "    s = np.random.uniform(size=(n, p - 1))\n",
    "    s = np.sort(s, axis=1)\n",
    "    k = np.ones(n) - s[:, -1]\n",
    "    j = np.diag(np.ones(p - 2), k=1)\n",
    "    s = s - np.dot(s, j)\n",
    "    W_init = np.c_[s, k]\n",
    "    check = np.sum(W_init, axis=1)\n",
    "    H_init = np.random.rand(p, np.shape(X_train)[1])\n",
    "\n",
    "    start_time = time.time()\n",
    "  \n",
    "    M_HALS, H_HALS, W_HALS, objective_HALS = nmf.mnmf(p, X_train, H_init, W_init, max_iter=5000)\n",
    "    \n",
    "    print(\"elapsed time: %5.2f seconds\" % (time.time() - start_time))\n",
    "    X_HALS = np.maximum(0, np.dot(W_HALS, H_HALS))\n",
    "    print('RRMSE =', nmf.rrmse(X_train, X_HALS))\n",
    "            \n",
    "    save_fileobj = ('matrix/objective_hals_rank_' + str(p) + '_entire.csv')\n",
    "    save_fileM = ('matrix/matrix_hals-m_rank_' + str(p) + '_entire.csv')\n",
    "    save_fileH = ('matrix/matrix_hals-h_rank_' + str(p) + '_entire.csv')\n",
    "    save_fileW = ('matrix/matrix_hals-w_rank_' + str(p) + '_entire.csv')\n",
    "            \n",
    "    np.savetxt(save_fileobj, objective_HALS, delimiter=',')\n",
    "    np.savetxt(save_fileM, M_HALS, delimiter=',')\n",
    "    np.savetxt(save_fileH, H_HALS, delimiter=',')\n",
    "    np.savetxt(save_fileW, W_HALS, delimiter=',')\n",
    "    \n",
    "    matrix = pd.read_csv('matrix/matrix_hals-w_rank_' + str(p) + '_entire.csv', delimiter=',', header=None)\n",
    "    W_HALS = matrix.values\n",
    "\n",
    "    matrix = pd.read_csv('matrix/matrix_hals-h_rank_' + str(p) + '_entire.csv', delimiter=',', header=None)\n",
    "    H_HALS = matrix.values\n",
    "\n",
    "    matrix = pd.read_csv('matrix/matrix_hals-m_rank_' + str(p) + '_entire.csv', delimiter=',', header=None)\n",
    "    M_HALS = matrix.values\n",
    "\n",
    "    model = AgglomerativeClustering(compute_full_tree=True, linkage=\"complete\", affinity='l1')\n",
    "    model.fit(W_HALS)\n",
    "    n_samples = n\n",
    "    minimal_size = p\n",
    "\n",
    "    if 'rtp1' in locals() or 'rtp1' in globals():\n",
    "        rtp1.clear()\n",
    "        \n",
    "    if 'rtp2' in locals() or 'rtp2' in globals():\n",
    "        rtp2.clear()\n",
    "    \n",
    "    rtp = []\n",
    "    rtp2 = nmf.tree_explorer(model, np.size(model.children_), n, minimal_size)\n",
    "    rtp1 = rtp2.copy()\n",
    "    \n",
    "    for i in range(len(rtp2)):\n",
    "        if (rtp2[i])[0] >= n_samples:\n",
    "            rtp1[i].pop(0)     \n",
    "    \n",
    "    number_of_clusters = len(rtp1)\n",
    "    labels = np.zeros(n)\n",
    "    \n",
    "    index_label = 0\n",
    "    for i in range(len(rtp1)):\n",
    "        for j in rtp1[i]:\n",
    "            labels[j] = index_label\n",
    "        index_label += 1\n",
    "\n",
    "    W_cluster_all = np.zeros((n, p, len(rtp1)))\n",
    "    \n",
    "    for i in range(len(rtp1)):\n",
    "        for j in rtp1[i]:\n",
    "            for k in range(p):\n",
    "                W_cluster_all[j, k, i] = W_HALS[j, k]\n",
    "    \n",
    "    W_cluster_final = [None] * len(rtp1)\n",
    "    \n",
    "    for i in range(len(rtp1)):\n",
    "        a = W_cluster_all[:, :, i]\n",
    "        W_cluster_final[i] = a[~(a==0).all(1)]\n",
    "    \n",
    "    X_cluster_all = np.zeros((n, d0, len(rtp1)))\n",
    "    \n",
    "    for i in range(len(rtp1)):\n",
    "        for j in rtp1[i]:\n",
    "            for k in range(d0):\n",
    "                X_cluster_all[j, k, i] = X_train[j, k]\n",
    "                \n",
    "    X_c = [None] * len(rtp1)\n",
    "    \n",
    "    for i in range(len(rtp1)):\n",
    "        a = X_cluster_all[:, :, i]\n",
    "        X_c[i] = a[~(a==0).all(1)]\n",
    "   \n",
    "    cluster_with_full_rank = []\n",
    "    cluster_small = []\n",
    "    \n",
    "    for i in range(len(rtp1)):\n",
    "        if len(rtp1[i]) == 1:    \n",
    "            cluster_small.append(i)\n",
    "        cluster_with_full_rank.append(i)\n",
    "\n",
    "    X_c_new = [None] * len(rtp1)\n",
    "    H_test = [None] * len(cluster_with_full_rank)\n",
    "    W_test = [None] * len(cluster_with_full_rank)\n",
    "    cluster_index = 0\n",
    "      \n",
    "    print('total number of clusters:', len(cluster_with_full_rank))\n",
    "    print('number of processes:', mp.cpu_count())\n",
    "\n",
    "    pool = mp.Pool(mp.cpu_count())        \n",
    "    X_c_new = pool.starmap(nmf.archetypes, [(X_c[i], p, i) for i in cluster_with_full_rank])\n",
    "    pool.close() \n",
    "\n",
    "    for cluster_index in cluster_with_full_rank:\n",
    "        matrix = pd.read_csv('matrix/H_test_cluster_' + str(p) + '_' + str(cluster_index) + '_entire.csv', delimiter=',', header=None)\n",
    "        H_test[cluster_index] = matrix.values\n",
    "        \n",
    "        matrix = pd.read_csv('matrix/W_test_cluster_' + str(p) + '_' + str(cluster_index) + '_entire.csv', delimiter=',', header=None)\n",
    "        W_test[cluster_index] = matrix.values\n",
    "        \n",
    "        X_c_new[cluster_index] = np.dot(W_test[cluster_index], H_test[cluster_index])\n",
    "\n",
    "    model = 'random_forest'\n",
    "    cluster_index = len(cluster_with_full_rank)\n",
    "\n",
    "    time_window_index = 0\n",
    "    H_pred = [None] * cluster_index\n",
    "    X_c_final = [None] * cluster_index\n",
    "\n",
    "    for time_window in time_windows_list: \n",
    "        for max_depth in max_depth_list:\n",
    "            \n",
    "            max_depth_list1 = []\n",
    "            max_depth_list1.append(max_depth)\n",
    "            \n",
    "            rdf = RandomForestRegressor(criterion='mse')\n",
    "            clf = GridSearchCV(estimator=rdf, param_grid=dict(max_depth=max_depth_list1), n_jobs=-1, scoring='neg_mean_squared_error')\n",
    "        \n",
    "            for c in range(cluster_index):\n",
    "                H_pred[c] = np.zeros((p, d-d0))\n",
    "                for i in range(p):    \n",
    "\n",
    "                    time_window = int(np.floor(time_window))\n",
    "                    #print('processing time window:', time_window)\n",
    "                    #print('processing max_depth:', max_depth)\n",
    "                    #print('processing archetype', i, 'of cluster', c, 'for rank', p)\n",
    "\n",
    "                    H_random_forest = (H_test[c])[i, :d0]\n",
    "                    X, y = nmf.split_sequence(H_random_forest, time_window, d-d0) \n",
    "                    clf.fit(X, y) \n",
    "\t\t\n",
    "                    H_test1 = (H_test[c])[i, d0-time_window:d0]\n",
    "                    H_test1 = H_test1.reshape((1, time_window))\n",
    "                    (H_pred[c])[i, :] = clf.predict(H_test1)\n",
    "                    #print('forecasts:', (H_pred[c])[i, :])\n",
    "                    \n",
    "                    save_fileH_pred = ('archetypes/matrix_hals-h_pred_' + str(time_window) + '_' + str(max_depth) + '_rank_' + str(p) + '_' + str(c) + '_' + str(i) + '_entire.csv')\n",
    "                    np.savetxt(save_fileH_pred, (H_pred[c])[i, :], delimiter=',')\n",
    "               \n",
    "                    matrix = pd.read_csv('archetypes/matrix_hals-h_pred_' + str(time_window) + '_' + str(max_depth) + '_rank_' + str(p) + '_' + str(c) + '_' + str(i) + '_entire.csv', delimiter=',', header=None)\t\t    \n",
    "                    (H_pred[c])[i, :] = matrix.values[:, 0]\n",
    "                \n",
    "                X_c_final[c] = np.maximum(0, np.dot(W_test[c], H_pred[c]))\n",
    "                \n",
    "            X_HALS_final = np.zeros((n, d-d0))            \n",
    "                    \n",
    "            for i in range(len(rtp1)):\n",
    "                index = 0\n",
    "                for j in rtp1[i]:\n",
    "                    for k in range(d-d0): \n",
    "                        X_HALS_final[j, k] = (X_c_final[i])[index, k]\n",
    "                    index += 1\n",
    "            \n",
    "            print('rank p:', p) \n",
    "            print('max_depth:', max_depth)\n",
    "            print('time_window:', time_window)\n",
    "\t    \n",
    "            print('rrmse:', nmf.rrmse(X_test, X_HALS_final))\n",
    "            print('mpe:', nmf.mpe(X_test, X_HALS_final))\n",
    "            print('l2:', nmf.l2_error(X_test, X_HALS_final))\n",
    "            print('l1:', nmf.l1_error(X_test, X_HALS_final))\n",
    "            \n",
    "            f.write('rank p: ' + str(p) + '\\n')\n",
    "            f.write('max_depth: ' + str(max_depth) + '\\n')\n",
    "            f.write('time_window: ' + str(time_window) + '\\n')\n",
    "            \n",
    "            f.write('rrmse: ' + str(nmf.rrmse(X_test, X_HALS_final)) + '\\n')\n",
    "            f.write('mpe: ' + str(nmf.mpe(X_test, X_HALS_final)) + '\\n')\n",
    "            f.write('l2: ' + str(nmf.l2_error(X_test, X_HALS_final)) + '\\n')\n",
    "            f.write('l1: ' + str(nmf.l1_error(X_test, X_HALS_final)) + '\\n\\n')\n",
    "            \n",
    "            if nmf.rrmse(X_test, X_HALS_final) < best_error:\n",
    "                p_best = p\n",
    "                max_depth_best = max_depth\n",
    "                time_window_best = time_window\n",
    "                X_forecast_best = X_HALS_final.copy()\n",
    "                best_error = nmf.rrmse(X_test, X_HALS_final)\n",
    "                \n",
    "save_file_forecast_best = ('matrix_forecast_best_entire.csv')\n",
    "np.savetxt(save_file_forecast_best, X_forecast_best, delimiter=',')\n",
    "                \n",
    "print('best rank p:', p_best) \n",
    "print('best max_depth:', max_depth_best)\n",
    "print('best time_window:', time_window_best)\n",
    "print('best RRMSE:', best_error)\n",
    "\n",
    "f.write('best rank p: ' + str(p_best) + '\\n')\n",
    "f.write('best max_depth: ' + str(max_depth_best) + '\\n')\n",
    "f.write('best time_window: ' + str(time_window_best) + '\\n\\n')\n",
    "                \n",
    "f.close()\n",
    "\n",
    "# %%\n",
    "\n",
    "trajectory_list = random.sample(range(n), 5)\n",
    "\n",
    "for i in trajectory_list:\n",
    "    nmf.trajectory_plot(i, X_test, X_forecast_best, p_best)\n",
    "    print(nmf.mpe(X_test[i, :], X_forecast_best[i, :]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Sliding Mask Method (SMM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, './sliding mask method')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a) mask Archetypal Matrix Factorization (mAMF) without overlapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import include_amf as amf\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "mask_dir = os.path.join(current_directory, r'mask_test')\n",
    "if not os.path.exists(mask_dir):\n",
    "    os.makedirs(mask_dir)\n",
    "trajectories_dir = os.path.join(current_directory, r'trajectories')\n",
    "if not os.path.exists(trajectories_dir):\n",
    "    os.makedirs(trajectories_dir)\n",
    "    \n",
    "# %%\n",
    "\n",
    "best_error = 1000000\n",
    "f = open(\"mask_test/output_file.out\", 'w')\n",
    "\n",
    "list_w = [4]\n",
    "list_rank = [5]\n",
    "\n",
    "for w in list_w:\n",
    "    print(\"w:\", w)\n",
    "    f.write('w: ' + str(w) +'\\n\\n')\n",
    "    \n",
    "    periodicity = 4\n",
    "    X_real = X_original.copy()\n",
    "    for i in range(n):\n",
    "        X_real[i, d0:d] = X_original[i, d0-periods_to_forecast:d0]\n",
    "    X_real = X_real.flatten(order='C')\n",
    "    \n",
    "    # %%\n",
    "    \n",
    "    b=d/(periodicity*w)\n",
    "    row = int(b*n)\n",
    "    \n",
    "    X_original_block = np.split(X_real, np.shape(X_real)[0]/periodicity)    \n",
    "    X_original_block_2 = np.zeros((row, w*periodicity))\n",
    "    tau_2 = np.zeros(row)\n",
    "    \n",
    "    jindex = 0\n",
    "    a = 0\n",
    "    for index in range(len(X_original_block)):\n",
    "        c = X_original_block[index]\n",
    "        X_original_block_2[jindex, a:a+periodicity] = c\n",
    "        a += periodicity\n",
    "        if a == w*periodicity:\n",
    "            jindex += 1\n",
    "            a = 0\n",
    "    \n",
    "    index2 = b-1\n",
    "    for jndex1 in range(int(row/b)):\n",
    "        index2 = int(index2)\n",
    "        tau_2[index2] = 1\n",
    "        index2 += b\n",
    "    \n",
    "    tau = np.argmax(X_original_block_2 != 0, axis=1)\n",
    "    \n",
    "    tau_1a = tau_2[(~np.all(X_original_block_2 == 0, axis=1)) | (tau_2==1)]\n",
    "    X_original_block_1 = X_original_block_2[(~np.all(X_original_block_2 == 0, axis=1)) | (tau_2==1)] \n",
    "    \n",
    "    # %%\n",
    "        \n",
    "    m = np.shape(X_original_block_1)[0]\n",
    "    \n",
    "    for p in list_rank:\n",
    "        np.random.seed(0)\n",
    "    \n",
    "        s = np.random.uniform(size=(m, p - 1))\n",
    "        s = np.sort(s, axis=1)\n",
    "        k = np.ones(m) - s[:, -1]\n",
    "        j = np.diag(np.ones(p - 2), k=1)\n",
    "        s = s - np.dot(s, j)\n",
    "        W_init = np.c_[s, k]\n",
    "        check = np.sum(W_init, axis=1)\n",
    "        \n",
    "        H_init = np.random.rand(p, w*periodicity)*100\n",
    "    \n",
    "        start_time = time.time()\n",
    "    \n",
    "        print(\"rank:\", p)\n",
    "    \n",
    "        M_HALS, H_HALS, W_HALS, objective_HALS = amf.mnmf(p, X_original_block_1, H_init, W_init, tau_1a, periods_to_forecast, max_iter=5000)\n",
    "        print('Error: %5.4f' % np.linalg.norm(M_HALS - np.dot(W_HALS, H_HALS)))\n",
    "        print(\"elapsed time: %5.2f seconds\" % (time.time() - start_time))\n",
    "    \n",
    "        save_fileobj = ('mask_test/objective_hals_rank_' + str(w) + '_' + str(p) +'.csv')\n",
    "        save_fileM = ('mask_test/matrix_hals-m_rank_' + str(w) + '_' + str(p) +'.csv')\n",
    "        save_fileH = ('mask_test/matrix_hals-h_rank_' + str(w) + '_' + str(p) +'.csv')\n",
    "        save_fileW = ('mask_test/matrix_hals-w_rank_' + str(w) + '_' + str(p) +'.csv')\n",
    "    \n",
    "        np.savetxt(save_fileobj, objective_HALS, delimiter=',')\n",
    "        np.savetxt(save_fileM, M_HALS, delimiter=',')\n",
    "        np.savetxt(save_fileH, H_HALS, delimiter=',')\n",
    "        np.savetxt(save_fileW, W_HALS, delimiter=',')\n",
    "    \n",
    "        matrix = pd.read_csv('mask_test/matrix_hals-w_rank_' + str(w) + '_' + str(p) +'.csv', delimiter=',', header=None)\n",
    "        W_HALS = matrix.values\n",
    "    \n",
    "        matrix = pd.read_csv('mask_test/matrix_hals-h_rank_' + str(w) + '_' + str(p) +'.csv', delimiter=',', header=None)\n",
    "        H_HALS = matrix.values\n",
    "    \n",
    "        matrix = pd.read_csv('mask_test/matrix_hals-m_rank_' + str(w) + '_' + str(p) +'.csv', delimiter=',', header=None)\n",
    "        M_HALS = matrix.values\n",
    "        \n",
    "        M_new_block = np.zeros((np.shape(X_original_block_2)[0], w*periodicity))\n",
    "        M_new_block[(~np.all(X_original_block_2 == 0, axis=1)) | (tau_2==1)] = M_HALS[:]\n",
    "        M_new = np.zeros((n, d))\n",
    "    \n",
    "        jindex = 0\n",
    "        a = 0\n",
    "        for index in range(np.shape(X_original_block_2)[0]):\n",
    "            c = M_new_block[index, :]\n",
    "            M_new[jindex, a:a+periodicity*w] = c\n",
    "            a += periodicity*w\n",
    "            if a == d:\n",
    "                jindex += 1\n",
    "                a = 0\n",
    "                \n",
    "        M_new = M_new[:, d0:d] \n",
    "        \n",
    "        prevision_error_HALS_rrmse = amf.rrmse(X_test, M_new)\n",
    "        prevision_error_HALS_l1 = amf.l1_error(X_test, M_new)\n",
    "        prevision_error_HALS_mpe = amf.mpe(X_test, M_new)\n",
    "        prevision_error_HALS_l2 = amf.l2_error(X_test, M_new)\n",
    "        \n",
    "        print('rrmse:', prevision_error_HALS_rrmse)\n",
    "        print('mpe:', prevision_error_HALS_mpe)\n",
    "        print('l2:', prevision_error_HALS_l2)\n",
    "        print('l1:', prevision_error_HALS_l1)  \n",
    "    \n",
    "        f.write('p: ' + str(p)  + '\\n')\n",
    "        f.write('rrmse: ' + str(prevision_error_HALS_rrmse)  + '\\n')\n",
    "        f.write('mpe: ' + str(prevision_error_HALS_mpe)  + '\\n')\n",
    "        f.write('l2: ' + str(prevision_error_HALS_l2)  + '\\n')\n",
    "        f.write('l1: ' + str(prevision_error_HALS_l1)  + '\\n')\n",
    "        f.write('\\n')   \n",
    "    \n",
    "        if amf.rrmse(X_test, M_new) <= best_error:\n",
    "            w_best = w\n",
    "            p_best = p\n",
    "            X_forecast_best = M_new.copy()\n",
    "            best_error = amf.rrmse(X_test, M_new)\n",
    "                    \n",
    "save_file_forecast_best = ('matrix_forecast_best_mask_test.csv')\n",
    "np.savetxt(save_file_forecast_best, X_forecast_best, delimiter=',')\n",
    "                    \n",
    "print('best rank w:', w_best) \n",
    "print('best rank p:', p_best) \n",
    "print('best RRMSE:', best_error)\n",
    "\n",
    "f.write('best rank w: ' + str(w_best) + '\\n')    \n",
    "f.write('best rank p: ' + str(p_best) + '\\n')\n",
    "f.write('best RRMSE: ' + str(best_error) + '\\n')\n",
    "                    \n",
    "f.close()\n",
    "    \n",
    "# %%\n",
    "    \n",
    "trajectory_list = random.sample(range(n), 5)\n",
    "\n",
    "for i in trajectory_list:\n",
    "    print(amf.rrmse(X_test[i, :], X_forecast_best[i, :]))\n",
    "    amf.trajectory_plot(i, X_test, X_forecast_best, p_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b) mask Archetypal Matrix Factorization (mAMF) with overlapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import include_amf_overlap as amf_overlap\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "mask_dir = os.path.join(current_directory, r'mask_test')\n",
    "if not os.path.exists(mask_dir):\n",
    "    os.makedirs(mask_dir)\n",
    "trajectories_dir = os.path.join(current_directory, r'trajectories')\n",
    "if not os.path.exists(trajectories_dir):\n",
    "    os.makedirs(trajectories_dir)\n",
    "    \n",
    "# %%\n",
    "\n",
    "best_error = 1000000\n",
    "f = open(\"mask_test/output_file.out\", 'w')\n",
    "\n",
    "list_w = [4]\n",
    "list_rank = [5]\n",
    "\n",
    "for w in list_w:\n",
    "    print(\"w:\", w)\n",
    "    f.write('w: ' + str(w) +'\\n\\n')\n",
    "    \n",
    "    periodicity = 4\n",
    "    X_real = X_original.copy()\n",
    "    for i in range(n):\n",
    "        X_real[i, d0:d] = X_original[i, d0-periods_to_forecast:d0]\n",
    "    X_real = X_real.flatten(order='C')\n",
    "    \n",
    "    # %%\n",
    "    \n",
    "    b=d/(periodicity*w)\n",
    "    row = int(b*n*2)\n",
    "    size = int(periodicity)\n",
    "    step = int(periodicity/2)\n",
    "\n",
    "    X_original_block = [X_real[i : i + size] for i in range(0, len(X_real), step)]\n",
    "    X_original_block_2 = np.zeros((row, w*periodicity))\n",
    "    tau_2 = np.zeros(row)\n",
    "\n",
    "    pp = int(periodicity/2)\n",
    "    \n",
    "    jindex = 0\n",
    "    a = 0\n",
    "    for index in range(len(X_original_block)-1):\n",
    "        c = X_original_block[index]\n",
    "        X_original_block_2[jindex, a:a+periodicity] = c\n",
    "        a += periodicity\n",
    "        if a == w*periodicity:\n",
    "            jindex += 1\n",
    "            a = 0\n",
    "    \n",
    "    index2 = b*2-1\n",
    "    for jndex1 in range(int(row/b/2)):\n",
    "        index2 = int(index2)\n",
    "        tau_2[index2] = 1\n",
    "        index2 += b*2\n",
    "\n",
    "    tau = np.argmax(X_original_block_2 != 0, axis=1)    \n",
    "    tau_1a = tau_2[(~np.all(X_original_block_2 == 0, axis=1)) | (tau_2==1)]\n",
    "    X_original_block_1 = X_original_block_2[(~np.all(X_original_block_2 == 0, axis=1)) | (tau_2==1)]\n",
    "\n",
    "    \n",
    "    # %%\n",
    "        \n",
    "    m = np.shape(X_original_block_1)[0]\n",
    "    \n",
    "    for p in list_rank:\n",
    "        np.random.seed(0)\n",
    "    \n",
    "        s = np.random.uniform(size=(m, p - 1))\n",
    "        s = np.sort(s, axis=1)\n",
    "        k = np.ones(m) - s[:, -1]\n",
    "        j = np.diag(np.ones(p - 2), k=1)\n",
    "        s = s - np.dot(s, j)\n",
    "        W_init = np.c_[s, k]\n",
    "        check = np.sum(W_init, axis=1)\n",
    "        \n",
    "        H_init = np.random.rand(p, w*periodicity)*100\n",
    "    \n",
    "        start_time = time.time()\n",
    "    \n",
    "        print(\"rank:\", p)\n",
    "    \n",
    "        M_HALS, H_HALS, W_HALS, objective_HALS = amf_overlap.mnmf(p, X_original_block_1, H_init, W_init, tau_1a, w, periodicity, periods_to_forecast, max_iter=5000)\n",
    "        print('Error: %5.4f' % np.linalg.norm(M_HALS - np.dot(W_HALS, H_HALS)))\n",
    "        print(\"elapsed time: %5.2f seconds\" % (time.time() - start_time))\n",
    "    \n",
    "        save_fileobj = ('mask_test/objective_hals_rank_' + str(w) + '_' + str(p) +'.csv')\n",
    "        save_fileM = ('mask_test/matrix_hals-m_rank_' + str(w) + '_' + str(p) +'.csv')\n",
    "        save_fileH = ('mask_test/matrix_hals-h_rank_' + str(w) + '_' + str(p) +'.csv')\n",
    "        save_fileW = ('mask_test/matrix_hals-w_rank_' + str(w) + '_' + str(p) +'.csv')\n",
    "    \n",
    "        np.savetxt(save_fileobj, objective_HALS, delimiter=',')\n",
    "        np.savetxt(save_fileM, M_HALS, delimiter=',')\n",
    "        np.savetxt(save_fileH, H_HALS, delimiter=',')\n",
    "        np.savetxt(save_fileW, W_HALS, delimiter=',')\n",
    "    \n",
    "        matrix = pd.read_csv('mask_test/matrix_hals-w_rank_' + str(w) + '_' + str(p) +'.csv', delimiter=',', header=None)\n",
    "        W_HALS = matrix.values\n",
    "    \n",
    "        matrix = pd.read_csv('mask_test/matrix_hals-h_rank_' + str(w) + '_' + str(p) +'.csv', delimiter=',', header=None)\n",
    "        H_HALS = matrix.values\n",
    "    \n",
    "        matrix = pd.read_csv('mask_test/matrix_hals-m_rank_' + str(w) + '_' + str(p) +'.csv', delimiter=',', header=None)\n",
    "        M_HALS = matrix.values\n",
    "        \n",
    "        M_new_block = np.zeros((np.shape(X_original_block_2)[0], w*periodicity))\n",
    "        M_new_block[(~np.all(X_original_block_2 == 0, axis=1)) | (tau_2==1)] =  M_HALS[:]\n",
    "        M_new = np.zeros((n, d-d0))\n",
    "        \n",
    "        jindex = 0\n",
    "        \n",
    "        for i in range(np.shape(M_new_block)[0]):\n",
    "            if tau_2[i] == 1:\n",
    "                M_new[jindex, :] = M_new_block[i, (w-1)*periodicity-periods_to_forecast:(w-1)*periodicity]\n",
    "                jindex += 1\n",
    "        \n",
    "        prevision_error_HALS_rrmse = amf.rrmse(X_test, M_new)\n",
    "        prevision_error_HALS_l1 = amf.l1_error(X_test, M_new)\n",
    "        prevision_error_HALS_mpe = amf.mpe(X_test, M_new)\n",
    "        prevision_error_HALS_l2 = amf.l2_error(X_test, M_new)\n",
    "        \n",
    "        print('rrmse:', prevision_error_HALS_rrmse)\n",
    "        print('mpe:', prevision_error_HALS_mpe)\n",
    "        print('l2:', prevision_error_HALS_l2)\n",
    "        print('l1:', prevision_error_HALS_l1)  \n",
    "    \n",
    "        f.write('p: ' + str(p)  + '\\n')\n",
    "        f.write('rrmse: ' + str(prevision_error_HALS_rrmse)  + '\\n')\n",
    "        f.write('mpe: ' + str(prevision_error_HALS_mpe)  + '\\n')\n",
    "        f.write('l2: ' + str(prevision_error_HALS_l2)  + '\\n')\n",
    "        f.write('l1: ' + str(prevision_error_HALS_l1)  + '\\n')\n",
    "        f.write('\\n')   \n",
    "    \n",
    "        if amf.rrmse(X_test, M_new) <= best_error:\n",
    "            w_best = w\n",
    "            p_best = p\n",
    "            X_forecast_best = M_new.copy()\n",
    "            best_error = amf.rrmse(X_test, M_new)\n",
    "                    \n",
    "save_file_forecast_best = ('matrix_forecast_best_mask_test.csv')\n",
    "np.savetxt(save_file_forecast_best, X_forecast_best, delimiter=',')\n",
    "                    \n",
    "print('best rank w:', w_best) \n",
    "print('best rank p:', p_best) \n",
    "print('best RRMSE:', best_error)\n",
    "\n",
    "f.write('best rank w: ' + str(w_best) + '\\n')    \n",
    "f.write('best rank p: ' + str(p_best) + '\\n')\n",
    "f.write('best RRMSE: ' + str(best_error) + '\\n')\n",
    "                    \n",
    "f.close()\n",
    "    \n",
    "# %%\n",
    "    \n",
    "trajectory_list = random.sample(range(n), 5)\n",
    "\n",
    "for i in trajectory_list:\n",
    "    print(amf.rrmse(X_test[i, :], X_forecast_best[i, :]))\n",
    "    amf.trajectory_plot(i, X_test, X_forecast_best, p_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2c) mask Nonnegative Matrix Factorization (mNMF) without overlapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import include_nmf as nmf\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "mask_dir = os.path.join(current_directory, r'mask_test')\n",
    "if not os.path.exists(mask_dir):\n",
    "    os.makedirs(mask_dir)\n",
    "trajectories_dir = os.path.join(current_directory, r'trajectories')\n",
    "if not os.path.exists(trajectories_dir):\n",
    "    os.makedirs(trajectories_dir)\n",
    "\n",
    "# %%\n",
    "\n",
    "# %%\n",
    "\n",
    "best_error = 1000000\n",
    "f = open(\"mask_test/output_file.out\", 'w')\n",
    "\n",
    "list_w = [4]\n",
    "list_rank = [5]\n",
    "\n",
    "for w in list_w:\n",
    "    print(\"w:\", w)\n",
    "    f.write('w: ' + str(w) +'\\n\\n')\n",
    "    \n",
    "    periodicity = 4\n",
    "    X_real = X_original.copy()\n",
    "    for i in range(n):\n",
    "        X_real[i, d0:d] = X_original[i, d0-periods_to_forecast:d0]\n",
    "    X_real = X_real.flatten(order='C')\n",
    "    \n",
    "    # %%\n",
    "    \n",
    "    b=d/(periodicity*w)\n",
    "    row = int(b*n)\n",
    "    \n",
    "    X_original_block = np.split(X_real, np.shape(X_real)[0]/periodicity)    \n",
    "    X_original_block_2 = np.zeros((row, w*periodicity))\n",
    "    tau_2 = np.zeros(row)\n",
    "    \n",
    "    jindex = 0\n",
    "    a = 0\n",
    "    for index in range(len(X_original_block)):\n",
    "        c = X_original_block[index]\n",
    "        X_original_block_2[jindex, a:a+periodicity] = c\n",
    "        a += periodicity\n",
    "        if a == w*periodicity:\n",
    "            jindex += 1\n",
    "            a = 0\n",
    "    \n",
    "    index2 = b-1\n",
    "    for jndex1 in range(int(row/b)):\n",
    "        index2 = int(index2)\n",
    "        tau_2[index2] = 1\n",
    "        index2 += b\n",
    "    \n",
    "    tau = np.argmax(X_original_block_2 != 0, axis=1)\n",
    "    \n",
    "    tau_1a = tau_2[(~np.all(X_original_block_2 == 0, axis=1)) | (tau_2==1)]\n",
    "    X_original_block_1 = X_original_block_2[(~np.all(X_original_block_2 == 0, axis=1)) | (tau_2==1)] \n",
    "    \n",
    "    # %%\n",
    "        \n",
    "    m = np.shape(X_original_block_1)[0]\n",
    "    \n",
    "    for p in list_rank:\n",
    "        np.random.seed(0)\n",
    "    \n",
    "        s = np.random.uniform(size=(m, p - 1))\n",
    "        s = np.sort(s, axis=1)\n",
    "        k = np.ones(m) - s[:, -1]\n",
    "        j = np.diag(np.ones(p - 2), k=1)\n",
    "        s = s - np.dot(s, j)\n",
    "        W_init = np.c_[s, k]\n",
    "        check = np.sum(W_init, axis=1)\n",
    "        \n",
    "        H_init = np.random.rand(p, w*periodicity)*100\n",
    "    \n",
    "        start_time = time.time()\n",
    "    \n",
    "        print(\"rank:\", p)\n",
    "    \n",
    "        M_HALS, H_HALS, W_HALS, objective_HALS = nmf.mnmf(p, X_original_block_1, H_init, W_init, tau_1a, periods_to_forecast, max_iter=5000)\n",
    "        print('Error: %5.4f' % np.linalg.norm(M_HALS - np.dot(W_HALS, H_HALS)))\n",
    "        print(\"elapsed time: %5.2f seconds\" % (time.time() - start_time))\n",
    "    \n",
    "        save_fileobj = ('mask_test/objective_hals_rank_' + str(w) + '_' + str(p) +'.csv')\n",
    "        save_fileM = ('mask_test/matrix_hals-m_rank_' + str(w) + '_' + str(p) +'.csv')\n",
    "        save_fileH = ('mask_test/matrix_hals-h_rank_' + str(w) + '_' + str(p) +'.csv')\n",
    "        save_fileW = ('mask_test/matrix_hals-w_rank_' + str(w) + '_' + str(p) +'.csv')\n",
    "    \n",
    "        np.savetxt(save_fileobj, objective_HALS, delimiter=',')\n",
    "        np.savetxt(save_fileM, M_HALS, delimiter=',')\n",
    "        np.savetxt(save_fileH, H_HALS, delimiter=',')\n",
    "        np.savetxt(save_fileW, W_HALS, delimiter=',')\n",
    "    \n",
    "        matrix = pd.read_csv('mask_test/matrix_hals-w_rank_' + str(w) + '_' + str(p) +'.csv', delimiter=',', header=None)\n",
    "        W_HALS = matrix.values\n",
    "    \n",
    "        matrix = pd.read_csv('mask_test/matrix_hals-h_rank_' + str(w) + '_' + str(p) +'.csv', delimiter=',', header=None)\n",
    "        H_HALS = matrix.values\n",
    "    \n",
    "        matrix = pd.read_csv('mask_test/matrix_hals-m_rank_' + str(w) + '_' + str(p) +'.csv', delimiter=',', header=None)\n",
    "        M_HALS = matrix.values\n",
    "        \n",
    "        M_new_block = np.zeros((np.shape(X_original_block_2)[0], w*periodicity))\n",
    "        M_new_block[(~np.all(X_original_block_2 == 0, axis=1)) | (tau_2==1)] = M_HALS[:]\n",
    "        M_new = np.zeros((n, d))\n",
    "    \n",
    "        jindex = 0\n",
    "        a = 0\n",
    "        for index in range(np.shape(X_original_block_2)[0]):\n",
    "            c = M_new_block[index, :]\n",
    "            M_new[jindex, a:a+periodicity*w] = c\n",
    "            a += periodicity*w\n",
    "            if a == d:\n",
    "                jindex += 1\n",
    "                a = 0\n",
    "                \n",
    "        M_new = M_new[:, d0:d] \n",
    "        \n",
    "        prevision_error_HALS_rrmse = amf.rrmse(X_test, M_new)\n",
    "        prevision_error_HALS_l1 = amf.l1_error(X_test, M_new)\n",
    "        prevision_error_HALS_mpe = amf.mpe(X_test, M_new)\n",
    "        prevision_error_HALS_l2 = amf.l2_error(X_test, M_new)\n",
    "        \n",
    "        print('rrmse:', prevision_error_HALS_rrmse)\n",
    "        print('mpe:', prevision_error_HALS_mpe)\n",
    "        print('l2:', prevision_error_HALS_l2)\n",
    "        print('l1:', prevision_error_HALS_l1)  \n",
    "    \n",
    "        f.write('p: ' + str(p)  + '\\n')\n",
    "        f.write('rrmse: ' + str(prevision_error_HALS_rrmse)  + '\\n')\n",
    "        f.write('mpe: ' + str(prevision_error_HALS_mpe)  + '\\n')\n",
    "        f.write('l2: ' + str(prevision_error_HALS_l2)  + '\\n')\n",
    "        f.write('l1: ' + str(prevision_error_HALS_l1)  + '\\n')\n",
    "        f.write('\\n')   \n",
    "    \n",
    "        if amf.rrmse(X_test, M_new) <= best_error:\n",
    "            w_best = w\n",
    "            p_best = p\n",
    "            X_forecast_best = M_new.copy()\n",
    "            best_error = amf.rrmse(X_test, M_new)\n",
    "                    \n",
    "save_file_forecast_best = ('matrix_forecast_best_mask_test.csv')\n",
    "np.savetxt(save_file_forecast_best, X_forecast_best, delimiter=',')\n",
    "                    \n",
    "print('best rank w:', w_best) \n",
    "print('best rank p:', p_best) \n",
    "print('best RRMSE:', best_error)\n",
    "\n",
    "f.write('best rank w: ' + str(w_best) + '\\n')    \n",
    "f.write('best rank p: ' + str(p_best) + '\\n')\n",
    "f.write('best RRMSE: ' + str(best_error) + '\\n')\n",
    "                    \n",
    "f.close()\n",
    "    \n",
    "# %%\n",
    "    \n",
    "trajectory_list = random.sample(range(n), 5)\n",
    "\n",
    "for i in trajectory_list:\n",
    "    print(amf.rrmse(X_test[i, :], X_forecast_best[i, :]))\n",
    "    amf.trajectory_plot(i, X_test, X_forecast_best, p_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2d) mask Nonnegative Matrix Factorization (mNMF) with overlapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import include_nmf_overlap as nmf_overlap\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "mask_dir = os.path.join(current_directory, r'mask_test')\n",
    "if not os.path.exists(mask_dir):\n",
    "    os.makedirs(mask_dir)\n",
    "trajectories_dir = os.path.join(current_directory, r'trajectories')\n",
    "if not os.path.exists(trajectories_dir):\n",
    "    os.makedirs(trajectories_dir)\n",
    "    \n",
    "# %%\n",
    "\n",
    "best_error = 1000000\n",
    "f = open(\"mask_test/output_file.out\", 'w')\n",
    "\n",
    "list_w = [4]\n",
    "list_rank = [5]\n",
    "\n",
    "for w in list_w:\n",
    "    print(\"w:\", w)\n",
    "    f.write('w: ' + str(w) +'\\n\\n')\n",
    "    \n",
    "    periodicity = 4\n",
    "    X_real = X_original.copy()\n",
    "    for i in range(n):\n",
    "        X_real[i, d0:d] = X_original[i, d0-periods_to_forecast:d0]\n",
    "    X_real = X_real.flatten(order='C')\n",
    "    \n",
    "    # %%\n",
    "    \n",
    "    b=d/(periodicity*w)\n",
    "    row = int(b*n*2)\n",
    "    size = int(periodicity)\n",
    "    step = int(periodicity/2)\n",
    "\n",
    "    X_original_block = [X_real[i : i + size] for i in range(0, len(X_real), step)]\n",
    "    X_original_block_2 = np.zeros((row, w*periodicity))\n",
    "    tau_2 = np.zeros(row)\n",
    "\n",
    "    pp = int(periodicity/2)\n",
    "    \n",
    "    jindex = 0\n",
    "    a = 0\n",
    "    for index in range(len(X_original_block)-1):\n",
    "        c = X_original_block[index]\n",
    "        X_original_block_2[jindex, a:a+periodicity] = c\n",
    "        a += periodicity\n",
    "        if a == w*periodicity:\n",
    "            jindex += 1\n",
    "            a = 0\n",
    "    \n",
    "    index2 = b*2-1\n",
    "    for jndex1 in range(int(row/b/2)):\n",
    "        index2 = int(index2)\n",
    "        tau_2[index2] = 1\n",
    "        index2 += b*2\n",
    "\n",
    "    tau = np.argmax(X_original_block_2 != 0, axis=1)    \n",
    "    tau_1a = tau_2[(~np.all(X_original_block_2 == 0, axis=1)) | (tau_2==1)]\n",
    "    X_original_block_1 = X_original_block_2[(~np.all(X_original_block_2 == 0, axis=1)) | (tau_2==1)]\n",
    "\n",
    "    \n",
    "    # %%\n",
    "        \n",
    "    m = np.shape(X_original_block_1)[0]\n",
    "    \n",
    "    for p in list_rank:\n",
    "        np.random.seed(0)\n",
    "    \n",
    "        s = np.random.uniform(size=(m, p - 1))\n",
    "        s = np.sort(s, axis=1)\n",
    "        k = np.ones(m) - s[:, -1]\n",
    "        j = np.diag(np.ones(p - 2), k=1)\n",
    "        s = s - np.dot(s, j)\n",
    "        W_init = np.c_[s, k]\n",
    "        check = np.sum(W_init, axis=1)\n",
    "        \n",
    "        H_init = np.random.rand(p, w*periodicity)*100\n",
    "    \n",
    "        start_time = time.time()\n",
    "    \n",
    "        print(\"rank:\", p)\n",
    "    \n",
    "        M_HALS, H_HALS, W_HALS, objective_HALS = nmf_overlap.mnmf(p, X_original_block_1, H_init, W_init, tau_1a, w, periodicity, periods_to_forecast, max_iter=5000)\n",
    "        print('Error: %5.4f' % np.linalg.norm(M_HALS - np.dot(W_HALS, H_HALS)))\n",
    "        print(\"elapsed time: %5.2f seconds\" % (time.time() - start_time))\n",
    "    \n",
    "        save_fileobj = ('mask_test/objective_hals_rank_' + str(w) + '_' + str(p) +'.csv')\n",
    "        save_fileM = ('mask_test/matrix_hals-m_rank_' + str(w) + '_' + str(p) +'.csv')\n",
    "        save_fileH = ('mask_test/matrix_hals-h_rank_' + str(w) + '_' + str(p) +'.csv')\n",
    "        save_fileW = ('mask_test/matrix_hals-w_rank_' + str(w) + '_' + str(p) +'.csv')\n",
    "    \n",
    "        np.savetxt(save_fileobj, objective_HALS, delimiter=',')\n",
    "        np.savetxt(save_fileM, M_HALS, delimiter=',')\n",
    "        np.savetxt(save_fileH, H_HALS, delimiter=',')\n",
    "        np.savetxt(save_fileW, W_HALS, delimiter=',')\n",
    "    \n",
    "        matrix = pd.read_csv('mask_test/matrix_hals-w_rank_' + str(w) + '_' + str(p) +'.csv', delimiter=',', header=None)\n",
    "        W_HALS = matrix.values\n",
    "    \n",
    "        matrix = pd.read_csv('mask_test/matrix_hals-h_rank_' + str(w) + '_' + str(p) +'.csv', delimiter=',', header=None)\n",
    "        H_HALS = matrix.values\n",
    "    \n",
    "        matrix = pd.read_csv('mask_test/matrix_hals-m_rank_' + str(w) + '_' + str(p) +'.csv', delimiter=',', header=None)\n",
    "        M_HALS = matrix.values\n",
    "        \n",
    "        M_new_block = np.zeros((np.shape(X_original_block_2)[0], w*periodicity))\n",
    "        M_new_block[(~np.all(X_original_block_2 == 0, axis=1)) | (tau_2==1)] =  M_HALS[:]\n",
    "        M_new = np.zeros((n, d-d0))\n",
    "        \n",
    "        jindex = 0\n",
    "        \n",
    "        for i in range(np.shape(M_new_block)[0]):\n",
    "            if tau_2[i] == 1:\n",
    "                M_new[jindex, :] = M_new_block[i, (w-1)*periodicity-periods_to_forecast:(w-1)*periodicity]\n",
    "                jindex += 1\n",
    "        \n",
    "        prevision_error_HALS_rrmse = amf.rrmse(X_test, M_new)\n",
    "        prevision_error_HALS_l1 = amf.l1_error(X_test, M_new)\n",
    "        prevision_error_HALS_mpe = amf.mpe(X_test, M_new)\n",
    "        prevision_error_HALS_l2 = amf.l2_error(X_test, M_new)\n",
    "        \n",
    "        print('rrmse:', prevision_error_HALS_rrmse)\n",
    "        print('mpe:', prevision_error_HALS_mpe)\n",
    "        print('l2:', prevision_error_HALS_l2)\n",
    "        print('l1:', prevision_error_HALS_l1)  \n",
    "    \n",
    "        f.write('p: ' + str(p)  + '\\n')\n",
    "        f.write('rrmse: ' + str(prevision_error_HALS_rrmse)  + '\\n')\n",
    "        f.write('mpe: ' + str(prevision_error_HALS_mpe)  + '\\n')\n",
    "        f.write('l2: ' + str(prevision_error_HALS_l2)  + '\\n')\n",
    "        f.write('l1: ' + str(prevision_error_HALS_l1)  + '\\n')\n",
    "        f.write('\\n')   \n",
    "    \n",
    "        if amf.rrmse(X_test, M_new) <= best_error:\n",
    "            w_best = w\n",
    "            p_best = p\n",
    "            X_forecast_best = M_new.copy()\n",
    "            best_error = amf.rrmse(X_test, M_new)\n",
    "                    \n",
    "save_file_forecast_best = ('matrix_forecast_best_mask_test.csv')\n",
    "np.savetxt(save_file_forecast_best, X_forecast_best, delimiter=',')\n",
    "                    \n",
    "print('best rank w:', w_best) \n",
    "print('best rank p:', p_best) \n",
    "print('best RRMSE:', best_error)\n",
    "\n",
    "f.write('best rank w: ' + str(w_best) + '\\n')    \n",
    "f.write('best rank p: ' + str(p_best) + '\\n')\n",
    "f.write('best RRMSE: ' + str(best_error) + '\\n')\n",
    "                    \n",
    "f.close()\n",
    "    \n",
    "# %%\n",
    "    \n",
    "trajectory_list = random.sample(range(n), 5)\n",
    "\n",
    "for i in trajectory_list:\n",
    "    print(amf.rrmse(X_test[i, :], X_forecast_best[i, :]))\n",
    "    amf.trajectory_plot(i, X_test, X_forecast_best, p_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Random Forest Regression (RFR)/Neural Network (NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, './random forest')\n",
    "import tensorflow as tf\n",
    "from keras.layers import SimpleRNN, LSTM, GRU, Bidirectional, Conv1D, MaxPooling1D, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import include_rfr as rfr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = np.argmax(X_original != 0, axis=1)\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "trajectories_dir = os.path.join(current_directory, r'trajectories')\n",
    "if not os.path.exists(trajectories_dir):\n",
    "    os.makedirs(trajectories_dir)\n",
    "    \n",
    "X_pred = np.zeros((n, d-d0))\n",
    "        \n",
    "for i in range(n):\n",
    "    np.random.seed(0)\n",
    "    tf.set_random_seed(1)\n",
    "    #print('processing MT:', i+1)\n",
    "    A = X_train[i, tau[i]:]\n",
    "    X_pred[i, :] = rfr.ext_archetype(d, d0, A, time_window=np.minimum(16, int(np.floor(np.shape(A)[0]/2))), model_dl=LSTM, cnn=False, rfr=True)  \n",
    "    #print('RRMSE =', rfr.rrmse(X_test[i, :], X_pred[i, :]))\n",
    "    #print('forecasts:', X_pred[i, :])\n",
    "    #print('test:', X_test[i, :])\n",
    "            \n",
    "print('RRMSE =', rfr.rrmse(X_test, X_pred))\n",
    "print('MPE =', rfr.mpe(X_test, X_pred))\n",
    "\n",
    "save_file_forecast_best = ('matrix_forecast_best_plain.csv')\n",
    "np.savetxt(save_file_forecast_best, X_pred, delimiter=',')\n",
    "\n",
    "# %%\n",
    "\n",
    "trajectory_list = random.sample(range(n), 5)\n",
    "\n",
    "for i in trajectory_list:\n",
    "    rfr.trajectory_plot(i, X_test, X_pred)\n",
    "    print(rfr.mpe(X_test[i, :], X_pred[i, :]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
